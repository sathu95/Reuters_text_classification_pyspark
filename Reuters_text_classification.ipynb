{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work\n",
      "fatal: destination path 'Reuters_text_classification_pyspark' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "# Going to the work directory \n",
    "%cd ~/notebook/work/\n",
    "\n",
    "# and cloning the project from github\n",
    "!git clone https://github.com/sathu95/Reuters_text_classification_pyspark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work/Reuters_text_classification_pyspark\n",
      "Already up-to-date.\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work/Reuters_text_classification_pyspark/Reuters\n",
      "bop\t coffee  dlr   jobs\t     nat-gas   ship\tveg-oil\n",
      "carcass  corn\t gnp   livestock     oilseed   soybean\tyen\n",
      "cocoa\t cpi\t gold  money-supply  reserves  sugar\n"
     ]
    }
   ],
   "source": [
    "# Now we have the project directory\n",
    "%cd ~/notebook/work/Reuters_text_classification_pyspark/\n",
    "# we can also pull the latest files from git\n",
    "!git pull\n",
    "%cd ./Reuters/\n",
    "!ls #checking all the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5\n",
      "('p: ', '/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work/Reuters_text_classification_pyspark/Reuters')\n",
      "{'cpi': 11, 'gnp': 2, 'jobs': 9, 'livestock': 6, 'gold': 3, 'yen': 4, 'bop': 8, 'corn': 15, 'carcass': 1, 'money-supply': 19, 'coffee': 5, 'sugar': 7, 'oilseed': 16, 'dlr': 10, 'veg-oil': 13, 'reserves': 0, 'soybean': 17, 'nat-gas': 12, 'ship': 14, 'cocoa': 18}\n",
      "CPU times: user 2.68 ms, sys: 3.78 ms, total: 6.45 ms\n",
      "Wall time: 369 ms\n",
      "('Number of documents read is:', 1913)\n",
      "CPU times: user 4.68 ms, sys: 1.69 ms, total: 6.37 ms\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work/Reuters_text_classification_pyspark/Reuters/reserves/0004940',\n",
       "  u\"\\n\\r\\nITALIAN NET RESERVES RISE IN FEBRUARY\\r\\n\\n    ROME, March 19 - Italy's net official reserves rose to\\r\\n66,172 billion lire in February 1987 from a previously reported\\r\\n62,174 billion in January, the Bank of Italy said.\\r\\n    Gold holdings at end-February totalled 35,203 billion lire,\\r\\nunchanged on January.\\r\\n    Convertible currencies totalled 18,467 billion lire, up\\r\\nfrom 14,899 billion in January, while European Currency Unit\\r\\n(ECU) holdings were 10,156 billion lire against 10,133 billion.\\r\\n\\r\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the necessary libraries\n",
    "\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#we get the absolute path for the working directory\n",
    "import os.path\n",
    "%cd ~\n",
    "p = os.path.abspath('./notebook/work/Reuters_text_classification_pyspark/Reuters/')\n",
    "print(\"p: \", p) \n",
    "\n",
    "# Creating a dictionary for class labels (from the directory names)\n",
    "topicLabels = {}\n",
    "import os\n",
    "label = 0\n",
    "# funtion to iterate through the working directories\n",
    "for root, dirs, files in os.walk(p, topdown=False):\n",
    "    for name in dirs:\n",
    "        topicLabels[name] = label # assigning a label number to each\n",
    "        label = label + 1 # and increment the label number by 1\n",
    "\n",
    "print(topicLabels)\n",
    "\n",
    "#checking the time using all the topics\n",
    "%time news_RDD = sc.wholeTextFiles(p + '/*') \n",
    "\n",
    "#To have a look at the total number of documents\n",
    "print ('Number of documents read is:',news_RDD.count())\n",
    "%time news_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'reserves', u\"\\n\\r\\nITALIAN NET RESERVES RISE IN FEBRUARY\\r\\n\\n    ROME, March 19 - Italy's net official reserves rose to\\r\\n66,172 billion lire in February 1987 from a previously reported\\r\\n62,174 billion in January, the Bank of Italy said.\\r\\n    Gold holdings at end-February totalled 35,203 billion lire,\\r\\nunchanged on January.\\r\\n    Convertible currencies totalled 18,467 billion lire, up\\r\\nfrom 14,899 billion in January, while European Currency Unit\\r\\n(ECU) holdings were 10,156 billion lire against 10,133 billion.\\r\\n\\r\", 0)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# new function to remove the headers using regular expressions\n",
    "def removeHeader(ft): \n",
    "    fn,text = ft # unpacking the filename and text content \n",
    "    matchObj = re.match(r'.+^Lines:(.*)', text,re.DOTALL|re.MULTILINE) \n",
    "    if(matchObj): # only if the pattern has matched \n",
    "        text = matchObj.group(1) # can we replace the text, \n",
    "    return (fn,text)\n",
    "\n",
    "ft_RDD = news_RDD.map(removeHeader)\n",
    "\n",
    "# Removing the path before the last directory name and the file name after (i.e. leaving the directory names, which are the Reuters news topics) \n",
    "tt_RDD = ft_RDD.map(lambda ft: (re.split('[/]',ft[0])[-2],ft[1]))\n",
    "\n",
    "\n",
    "# adding the topic numbers as labels\n",
    "# adding the labels, but adding a third component to each element. \n",
    "# This third element is determined by reading the label from the topicLabels dictionary, using the topic string (first in the RDD elements) as key.\n",
    "\n",
    "ttl_RDD = tt_RDD.map(lambda ft: (ft[0],ft[1],topicLabels[ft[0]]))\n",
    "\n",
    "# checking the result of above \n",
    "print(ttl_RDD.take(1))\n",
    "# it should contain [('topic', 'text', 'label')]\n",
    "\n",
    "# Saving the RDD as a pickle file for later use \n",
    "# ttl_RDD.saveAsPickleFile('ttl_RDD_full0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "clusterby": "text",
      "handlerId": "pieChart",
      "keyFields": "topic",
      "rowCount": "100000",
      "title": "Document frequency",
      "valueFields": "label"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# The schema is represented as a StructField object that comprises three fields, name (a string), dataType (a DataType) and nullable (a bool). \n",
    "# Creating 3 fields of names and types according to our data: two strings, one integer for thelabel. \n",
    "\n",
    "fields = [] \n",
    "fields.append(StructField('topic', StringType(), True))\n",
    "#now do the same for 'text' instead of topic:\n",
    "fields.append(StructField('text', StringType(), True))\n",
    "#and now for 'label' with type IntegerType instead of StringType:\n",
    "fields.append(StructField('label', IntegerType(), True))\n",
    "\n",
    "# All the fields above together define the schema \n",
    "schema = StructType(fields)\n",
    "\n",
    "# loading previously saved RDD\n",
    "ttl_RDD = sc.pickleFile('ttl_RDD_full0.pkl')\n",
    "# Applying the schema in createDataFrame, to create a DataFrame 'df' from the RDD\n",
    "df = sqlContext.createDataFrame(ttl_RDD, schema)\n",
    "\n",
    "#print the schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "#displaying the dataframe\n",
    "import pixiedust\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training-set count:', 1548)\n",
      "CPU times: user 4.87 ms, sys: 284 Âµs, total: 5.15 ms\n",
      "Wall time: 2.13 s\n",
      "('Test-set count:', 365)\n",
      "CPU times: user 2.87 ms, sys: 1.92 ms, total: 4.79 ms\n",
      "Wall time: 348 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[topic: string, text: string, label: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chnaging dataframe variable \n",
    "df2 = df\n",
    "\n",
    "# saving the datframe as a parquet, but once the file exists, this method will throw an error.\n",
    "#df.write.parquet('df_full0.pqt') \n",
    "\n",
    "#Creating the training and test set from the dataframe above by randomsplit<test = 20% and training = 80%>\n",
    "train_set, test_set = df2.randomSplit([0.8, 0.2])\n",
    "\n",
    "# caching the dataframe to make sure the sets are stored in memory or disk, rather than re-computed.\n",
    "train_set.cache()\n",
    "test_set.cache()\n",
    "# train_set.write.parquet('train_set0.pqt')\n",
    "# test_set.write.parquet('test_set0.pqt')\n",
    "\n",
    "#printing the counts of training ans test set and time the execution with the %time magic\n",
    "%time print (\"Training-set count:\",train_set.count())\n",
    "%time print (\"Test-set count:\",test_set.count())\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/se1e-c1219d6287d998-c60095ba54e5/notebook/work\n",
      "+++ all DataFrames loaded +++\n",
      "CPU times: user 10 ms, sys: 2.36 ms, total: 12.4 ms\n",
      "Wall time: 700 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Timing the execution of whole cell\n",
    "%cd ~/notebook/work/\n",
    "# df = spark.read.parquet('df0.pqt')\n",
    "train_set = spark.read.parquet('train_set0.pqt')\n",
    "test_set = spark.read.parquet('test_set0.pqt')\n",
    "print(\"+++ all DataFrames loaded +++\")\n",
    "\n",
    "# test and training sets are cached, equvalent to cache(), to increase the performance\n",
    "train_set.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_set.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pipeline:', 'stages: a list of pipeline stages (current: [Tokenizer_432ab6f3a08327391590, StopWordsRemover_4602814c5d94625320b1, HashingTF_453988508c99acf4efb9, IDF_4d4f87e50de68d724325, NaiveBayes_494db7e965e967464e98])')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression, RandomForestClassifier, NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
    "\n",
    "##Constructing a pipeline\n",
    "#By spliting each sentence by white spaces into words using Tokenizer. \n",
    "\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "#Removing stopwords\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "\n",
    "#using HashingTF to hash the sentence into a feature vector by bag of words \n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "\n",
    "#using IDF to rescale the feature vectors; this generally improves performance when using text as features.\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "\n",
    "#The feature vectors could then be passed through a learning algorithm, in this case Naive Bayes as cf\n",
    "cf = NaiveBayes()\n",
    "\n",
    "#connecting all the steps above to create one pipeline:\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, cf])\n",
    "print (\"Pipeline:\",pipeline.explainParams()) #Shows all the parameters used by above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tokenizer:', u'inputCol: input column name. (current: text)\\noutputCol: output column name. (default: Tokenizer_432ab6f3a08327391590__output, current: words)')\n",
      "\n",
      "\n",
      "\n",
      "('Remover:', u\"caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\\ninputCol: input column name. (current: words)\\noutputCol: output column name. (default: StopWordsRemover_4602814c5d94625320b1__output, current: filtered)\\nstopWords: The words to be filtered out (default: [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'])\")\n",
      "\n",
      "\n",
      "\n",
      "('HashingTF:', u'binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\\ninputCol: input column name. (current: filtered)\\nnumFeatures: number of features. (default: 262144, current: 1000)\\noutputCol: output column name. (default: HashingTF_453988508c99acf4efb9__output, current: rawFeatures)')\n",
      "\n",
      "\n",
      "\n",
      "('IDF:', u'inputCol: input column name. (current: rawFeatures)\\nminDocFreq: minimum number of documents in which a term should appear for filtering (default: 0, current: 0)\\noutputCol: output column name. (default: IDF_4d4f87e50de68d724325__output, current: features)')\n",
      "\n",
      "\n",
      "\n",
      "('classifier:', \"featuresCol: features column name. (default: features)\\nlabelCol: label column name. (default: label)\\nmodelType: The model type which is a string (case-sensitive). Supported options: multinomial (default) and bernoulli. (default: multinomial)\\npredictionCol: prediction column name. (default: prediction)\\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\\nsmoothing: The smoothing parameter, should be >= 0, default is 1.0 (default: 1.0)\\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\")\n"
     ]
    }
   ],
   "source": [
    "#getting information for each parameter  using the .explainParams()\n",
    "print (\"Tokenizer:\",tokenizer.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"Remover:\",remover.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"HashingTF:\",hashingTF.explainParams())\n",
    "print (\"\\n\\n\")\n",
    "print (\"IDF:\",idf.explainParams())\n",
    "print (\"\\n\\n\")\n",
    "print (\"classifier:\",cf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 ms, sys: 10.4 ms, total: 42.5 ms\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "# Fitting ML pipeline to the training data \n",
    "# and obtaining a trained pipeline model that can be used for prediction.\n",
    "%time model=pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|topic|prediction|label|\n",
      "+-----+----------+-----+\n",
      "|  yen|      10.0|    4|\n",
      "|  yen|      10.0|    4|\n",
      "|  yen|      10.0|    4|\n",
      "|  yen|      10.0|    4|\n",
      "|  yen|       4.0|    4|\n",
      "+-----+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+----------+-----+\n",
      "|topic|prediction|label|\n",
      "+-----+----------+-----+\n",
      "| corn|      15.0|   15|\n",
      "| corn|      14.0|   15|\n",
      "| corn|      15.0|   15|\n",
      "| corn|      17.0|   15|\n",
      "| corn|      15.0|   15|\n",
      "+-----+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the performance of the fitted pipeline model by displaying the predicted labels\n",
    "\n",
    "#using the .transform() on the test set to make predictions on the test set\n",
    "test_predictions = model.transform(test_set)\n",
    "train_predictions = model.transform(train_set)\n",
    "\n",
    "#Showing the predicted labels of topic \"yen\" along with true labels and raw texts.\n",
    "test_predictions.select(\"topic\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"%yen%\")).show(5)\n",
    "# Similarly for \"corn\"\n",
    "test_predictions.select(\"topic\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"%corn%\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy - training:', 0.8276762402088773)\n",
      "('Accuracy - testing:', 0.6325459317585301)\n"
     ]
    }
   ],
   "source": [
    "#Using the accuracy\n",
    "evaluator = MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "\n",
    "print (\"Accuracy - training:\",evaluator.evaluate(train_predictions))\n",
    "print (\"Accuracy - testing:\",evaluator.evaluate(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "\n",
    "#With 3 values for hashingTF.numFeatures and 3 values for idf,\n",
    "# this grid will have 3 x 3 = 9 parameter settings for the Validator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(hashingTF.numFeatures,[100,1000,10000])\\\n",
    "    .addGrid(idf.minDocFreq,[0,10,100])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 827 ms, sys: 210 ms, total: 1.04 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit \n",
    "tvs = TrainValidationSplit().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "# A validator requires an Estimator, a grid of Paramters, and an Evaluator.\n",
    "%time tvsModel = tvs.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy for tuned model =', 0.8557441253263708)\n",
      "('Test accuracy for tuned model =', 0.6351706036745407)\n",
      "('Test accuracy for default model:', 0.6325459317585301)\n"
     ]
    }
   ],
   "source": [
    "# calculating the training and test accuracy for the tuned model\n",
    "print(\"Training accuracy for tuned model =\",evaluator.evaluate(tvsModel.transform(train_set)))\n",
    "print(\"Test accuracy for tuned model =\",evaluator.evaluate(tvsModel.transform(test_set)))\n",
    "print (\"Test accuracy for default model:\",evaluator.evaluate(test_predictions))\n",
    "\n",
    "\n",
    "#we can see a 3% increase in the training accuracy of the tuned model, where as test accuracy of tuned model is close to that of the deafault "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pipeline:', 'stages: a list of pipeline stages (current: [Tokenizer_45d4b83feba3ebf98581, HashingTF_406b9043ac47d7529aeb, IDF_4f10b7b0f10a4d6e39f5, LogisticRegression_42a19c49d14bfe0c4425])')\n",
      "CPU times: user 28.7 ms, sys: 6.37 ms, total: 35.1 ms\n",
      "Wall time: 861 ms\n",
      "CPU times: user 751 ms, sys: 261 ms, total: 1.01 s\n",
      "Wall time: 11.5 s\n",
      "('Training accuracy for tuned model =', 0.8191031197158156)\n",
      "('Test accuracy for tuned model =', 0.7931578537565427)\n",
      "('Test accuracy for default model:', 0.610901237935405)\n"
     ]
    }
   ],
   "source": [
    "#Implementing a parameter grid (using pyspark.ml.tuning.ParamGridBuilder),\n",
    "#varying at least one feature preprocessing step, one machine learning parameter, and\n",
    "#the training set size \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#Creating the training and test set from the dataframe above by randomsplit<test = 30% and training = 70%>\n",
    "train_set1, test_set1 = df2.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "#using HashingTF to hash the sentence into a feature vector by bag of words \n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "\n",
    "#using IDF to rescale the feature vectors; this generally improves performance when using text as features.\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "pipeline1 = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "print (\"Pipeline:\",pipeline1.explainParams())\n",
    "\n",
    "# Fitting ML pipeline to the training data \n",
    "# and obtaining a trained pipeline model that can be used for prediction.\n",
    "%time model=pipeline.fit(train_set1)\n",
    "\n",
    "#using the .transform() on the test set to make predictions on the test set\n",
    "test_predictions = model.transform(test_set1)\n",
    "train_predictions = model.transform(train_set1)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "# A validator requires an Estimator, a grid of Paramters, and an Evaluator.\n",
    "%time tvsModel1 = tvs.fit(train_set1)\n",
    "\n",
    "# calculating the training and test accuracy for the tuned model\n",
    "print(\"Training accuracy for tuned model =\",evaluator.evaluate(tvsModel.transform(train_set1)))\n",
    "print(\"Test accuracy for tuned model =\",evaluator.evaluate(tvsModel.transform(test_set1)))\n",
    "print (\"Test accuracy for default model:\",evaluator.evaluate(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the stopwords were not removed and Logistic regression was applied with training and test set split as 70% and 30% respectively, \n",
    "#the training accuracy for tuned model gave 82% of accuracy and test accuracy came upto 79%,\n",
    "#which is better than the previous model which gave 63%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
